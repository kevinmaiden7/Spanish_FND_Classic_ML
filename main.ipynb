{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from data_preprocessing import get_matrix\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import plot_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/Merged/spanish_dataset.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up Data Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_length = 10000\n",
    "language = 'spanish'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, df = get_matrix(data = dataset, vocabulary_length = max_vocab_length, \n",
    "                        stemming = True, remove_stopwords = True, language = language)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df # dataset with normalized news, without stopwords and with stemming applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0]) # tf-idf representation sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Split and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 2 # set random seed for consistency and reproducibility of results\n",
    "test_size = 0.1 # test set\n",
    "dev_size = 0.1 # development (validation) set\n",
    "iterations = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = dataset.label.values\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = test_size, random_state = random_seed) # train/test\n",
    "print('train/test:', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_grid(model, parameters, pca, X_transformed_train):\n",
    "    \n",
    "    indexes = ShuffleSplit(n_splits = iterations, test_size = dev_size, random_state = random_seed) # train/dev\n",
    "    \n",
    "    acc = make_scorer(accuracy_score)\n",
    "    scores = {'acc': acc}\n",
    "    \n",
    "    clf = GridSearchCV(model, parameters, scoring = scores, cv = indexes, \n",
    "                            return_train_score = True, refit = 'acc', verbose = 1)\n",
    "    if pca:\n",
    "        clf.fit(X_transformed_train, Y_train)\n",
    "    else:\n",
    "        clf.fit(X_train, Y_train) \n",
    "    \n",
    "    outcomes = pd.DataFrame(clf.cv_results_)\n",
    "    outcomes = outcomes[['params', 'mean_train_acc', 'mean_test_acc', 'std_test_acc']]\n",
    "    \n",
    "    return outcomes, clf.best_estimator_, round(clf.best_score_, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier()\n",
    "parameters_RF = {'n_estimators': [100, 200, 300, 500], \n",
    "                     'max_features': [50, 100, 150]}\n",
    "\n",
    "SVC = SVC()\n",
    "parameters_SVC = {'kernel': ['linear', 'rbf'],\n",
    "                  'C': [1e3, 1, 0.001],\n",
    "                  'gamma': [0.1, 1]}\n",
    "\n",
    "MLPC = MLPClassifier(activation = 'relu', solver = 'adam')\n",
    "parameters_MLPC = {'hidden_layer_sizes': [(10),(50),(10,10),(50,50),(10,10,10),(50,50,50)],\n",
    "                   'max_iter': [1000,1500]}\n",
    "\n",
    "models = {RF: parameters_RF,\n",
    "          SVC: parameters_SVC,\n",
    "          MLPC: parameters_MLPC}\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for model, parameters in models.items():\n",
    "    print(model)\n",
    "    outcomes, best_estimator, best_score = execute_grid(model, parameters, pca = False, X_transformed_train = None)\n",
    "    print(outcomes.to_string())\n",
    "    results[best_estimator] = best_score\n",
    "    print(best_estimator, best_score)\n",
    "    print('')\n",
    "\n",
    "print('Best estimator per model:')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_two = list({k: results[k] for k in sorted(results, key=results.get, reverse=True)})[:2]\n",
    "best_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reported performance for the two best models\n",
    "\n",
    "print('Reported performance for the two best models\\n')\n",
    "for model in best_two:\n",
    "    Y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(Y_test, Y_pred)\n",
    "    print(str(model) + ' : ' + str(round(acc, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits = iterations, test_size = dev_size, random_state = random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = best_two[0]\n",
    "plot_utils.plot_learning_curve(estimator, estimator.__class__.__name__, X_train, Y_train, \n",
    "                   cv = cv, scoring = make_scorer(accuracy_score), ylim = (0,1.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = best_two[1]\n",
    "plot_utils.plot_learning_curve(estimator, estimator.__class__.__name__, X_train, Y_train, \n",
    "                   cv = cv, scoring = make_scorer(accuracy_score), ylim = (0,1.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cumulative Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_var_perc = 0.95\n",
    "\n",
    "try:\n",
    "    pca = PCA().fit(X_train.toarray()) # kept all components to evaluate cumulative explained variance\n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    cumulative_variance = map(lambda x: round(x, 2), cumulative_variance) # truncate to 2 floating digits\n",
    "    n_components = list(cumulative_variance).index(cumulative_var_perc)\n",
    "    print('Number of components for %.2f cumulative explained variance:'%cumulative_var_perc, n_components)\n",
    "except np.linalg.LinAlgError:\n",
    "    print('PCA did not converge. Try again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments applying PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = n_components)\n",
    "pca.fit(X_train.toarray()) # Fit with only training data\n",
    "X_transformed_train = pca.transform(X_train.toarray())\n",
    "X_transformed_test = pca.transform(X_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pca = {}\n",
    "\n",
    "for model, parameters in models.items():\n",
    "    print(model)\n",
    "    outcomes, best_estimator, best_score = execute_grid(model, parameters, pca = True, X_transformed_train = X_transformed_train)\n",
    "    print(outcomes.to_string())\n",
    "    results_pca[best_estimator] = best_score\n",
    "    print(best_estimator, best_score)\n",
    "    print('')\n",
    "\n",
    "print('Best estimator per model:\\n')\n",
    "results_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_two_pca = list({k: results_pca[k] for k in sorted(results_pca, key=results_pca.get, reverse=True)})[:2]\n",
    "best_two_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reported performance for the two best models with PCA\n",
    "\n",
    "print('Reported performance for the two best models after applying PCA\\n')\n",
    "for model in best_two_pca:\n",
    "    Y_pred = model.predict(X_transformed_test) # predict on transformed test data\n",
    "    acc = accuracy_score(Y_test, Y_pred)\n",
    "    print(str(model) + ' : ' + str(round(acc, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits = iterations, test_size = dev_size, random_state = random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = best_two_pca[0]\n",
    "plot_utils.plot_learning_curve(estimator, estimator.__class__.__name__, X_transformed_train, Y_train, \n",
    "                   cv = cv, scoring = make_scorer(accuracy_score), ylim = (0,1.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = best_two_pca[1]\n",
    "plot_utils.plot_learning_curve(estimator, estimator.__class__.__name__, X_transformed_train, Y_train, \n",
    "                   cv = cv, scoring = make_scorer(accuracy_score), ylim = (0,1.1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
